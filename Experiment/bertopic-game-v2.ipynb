{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f2a36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/divaoncom/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/divaoncom/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Sekarang bisa download\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a0a399",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'OpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepresentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyBERTInspired\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/bertopic/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[1;32m      5\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertopic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m ]\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/bertopic/_bertopic.py:51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plotting\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCluster\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedder\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepresentation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mmr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mmr\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_backend\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/bertopic/backend/__init__.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# OpenAI Embeddings\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIBackend\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install openai` \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/bertopic/backend/_openai.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Mapping, Any\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedder\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOpenAIBackend\u001b[39;00m(BaseEmbedder):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124;03m\"\"\"OpenAI Embedding Model.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     36\u001b[0m         client: openai\u001b[38;5;241m.\u001b[39mOpenAI,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m         generator_kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     41\u001b[0m     ):\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/bertopic/backend/_openai.py:36\u001b[0m, in \u001b[0;36mOpenAIBackend\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOpenAIBackend\u001b[39;00m(BaseEmbedder):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124;03m\"\"\"OpenAI Embedding Model.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m---> 36\u001b[0m         client: \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m,\n\u001b[1;32m     37\u001b[0m         embedding_model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-ada-002\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m         delay_in_seconds: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m         batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m         generator_kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     41\u001b[0m     ):\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m client\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Unduh resource NLTK\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(\"Cleaned_Reviews.csv\")\n",
    "\n",
    "# Stopwords gabungan\n",
    "nltk_stopwords = set(stopwords.words(\"english\"))\n",
    "custom_stopwords = nltk_stopwords.union({\n",
    "    \"game\", \"games\", \"player\", \"play\", \"played\", \"playing\", \"fun\", \"good\", \"great\", \"awesome\", \"cool\"\n",
    "})\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", str(text).lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in custom_stopwords and len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Seed Topics\n",
    "seed_topic_list = [\n",
    "    [\"gameplay\", \"mechanics\", \"combat system\", \"interaction\", \"controls\", \"gameplay features\", \"game flow\"],\n",
    "    [\"bugs\", \"lag\", \"fps\", \"frame rate\", \"crashes\", \"performance issues\", \"optimization\", \"game stability\"],\n",
    "    [\"score\", \"leaderboard\", \"ranking\", \"points system\", \"levels\", \"scoreboard\", \"achievements\"],\n",
    "    [\"multiplayer\", \"co-op\", \"community\", \"online play\", \"player interaction\", \"multiplayer mode\", \"social features\"],\n",
    "    [\"character design\", \"npc\", \"character customization\", \"avatars\", \"design features\", \"customizable characters\"],\n",
    "    [\"soundtrack\", \"bgm\", \"audio design\", \"background music\", \"sound effects\", \"melody\", \"soundtrack quality\"],\n",
    "    [\"plot\", \"narrative\", \"storyline\", \"story arc\", \"game plot\", \"ending\", \"character development\", \"game narrative\"]\n",
    "]\n",
    "\n",
    "# Embedding model & vectorizer\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "vectorizer_model = CountVectorizer(stop_words=list(custom_stopwords), ngram_range=(1, 2))\n",
    "\n",
    "# BERTopic Model\n",
    "representation_model = KeyBERTInspired()\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    seed_topic_list=seed_topic_list,\n",
    "    language=\"english\",\n",
    "    verbose=True,\n",
    "    min_topic_size=20,\n",
    "    representation_model=representation_model\n",
    ")\n",
    "\n",
    "# Proses dokumen\n",
    "documents = df[\"Cleaned_Review\"].astype(str).tolist()\n",
    "topics, _ = topic_model.fit_transform(documents)\n",
    "df[\"Topic\"] = topics\n",
    "\n",
    "# Tokenisasi untuk evaluasi coherence\n",
    "tokenized_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# ===== Simpan model & data =====\n",
    "with open(\"topic_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(topic_model, f)\n",
    "\n",
    "with open(\"tokenized_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenized_docs, f)\n",
    "\n",
    "df.to_csv(\"DataPerGameTopics.csv\", index=False)\n",
    "print(\"📁 Model dan tokenisasi disimpan. Siap untuk perhitungan coherence di Cell 2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3014b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KeyBERT\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# Fungsi untuk melakukan labeling topik menggunakan KeyBERT\n",
    "def label_topics_with_keybert(model, documents, n_keywords=3):\n",
    "    topic_labels = []\n",
    "    kw_model = KeyBERT()\n",
    "\n",
    "    # Iterasi melalui semua topik yang dihasilkan dan dokumentasi\n",
    "    for topic_id in range(model.get_topic_info().shape[0]):\n",
    "        # Ambil kata-kata dari setiap topik\n",
    "        topic_words = model.get_topic(topic_id)\n",
    "        \n",
    "        # Ambil kata-kata kunci yang relevan dari topik\n",
    "        top_keywords = [word for word, _ in topic_words]\n",
    "        \n",
    "        # Gunakan KeyBERT untuk menghasilkan label berdasarkan kata kunci topik\n",
    "        label = kw_model.extract_keywords(\" \".join(top_keywords), top_n=n_keywords, use_mmr=True, diversity=0.7)\n",
    "        \n",
    "        # Buat label untuk topik berdasarkan kata kunci yang diekstrak\n",
    "        topic_label = \", \".join([word for word, _ in label])\n",
    "        topic_labels.append(topic_label)\n",
    "        \n",
    "    return topic_labels\n",
    "\n",
    "# Load model yang sudah disimpan\n",
    "with open(\"topic_model.pkl\", \"rb\") as f:\n",
    "    topic_model = pickle.load(f)\n",
    "\n",
    "# Proses untuk memberikan label topik menggunakan KeyBERT\n",
    "try:\n",
    "    # Menghasilkan label topik menggunakan KeyBERT\n",
    "    topic_labels = label_topics_with_keybert(topic_model, documents)\n",
    "    \n",
    "    # Setel label ke model\n",
    "    topic_model.set_topic_labels(topic_labels)\n",
    "    print(\"✅ LLM-based topic labeling menggunakan KeyBERT berhasil.\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Gagal membuat label LLM dengan KeyBERT: \", e)\n",
    "\n",
    "# Simpan model dengan label topik baru\n",
    "with open(\"topic_model_with_labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump(topic_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a45ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung coherence score menggunakan BERTopic\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Menghitung coherence score\n",
    "coherence_score = topic_model.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score: {coherence_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
