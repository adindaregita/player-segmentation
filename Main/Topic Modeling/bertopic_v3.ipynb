{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2cced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796    obviously game pretty similar operator new mec...\n",
      "Name: cleaned_Reviews, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(merged_df[merged_df[\"Game\"] == \"112 Operator\"]['cleaned_Reviews'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75950ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game</th>\n",
       "      <th>Top_Clusters</th>\n",
       "      <th>Top_Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>112 Operator</td>\n",
       "      <td>[585]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Game Top_Clusters Top_Topic_Keywords\n",
       "25  112 Operator        [585]                 []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords_df[topic_keywords_df[\"Game\"] == \"112 Operator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c738ce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5629c1531ae541be8dc2f2954f2d3871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 22:11:05,354 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-07-23 22:11:07,471 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-07-23 22:11:07,472 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-07-23 22:11:07,698 - BERTopic - Cluster - Completed ✓\n",
      "2025-07-23 22:11:07,703 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-23 22:11:07,875 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(\"embedding_umap.csv\")\n",
    "df = df.dropna(subset=[\"cleaned_Reviews\"])\n",
    "documents = df[\"cleaned_Reviews\"].tolist()\n",
    "\n",
    "# 2. Siapkan embedding\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
    "\n",
    "# 3. Inisialisasi model BERTopic\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom')\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4. Fit dan transform\n",
    "topics, probs = topic_model.fit_transform(documents, embeddings)\n",
    "\n",
    "# 5. Manual Top Keyword dengan MMR\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n=10, diversity=0.7):\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding.reshape(1, -1))\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "    \n",
    "    selected_words = []\n",
    "    selected_idxs = []\n",
    "    remaining_idxs = list(range(len(words)))\n",
    "\n",
    "    selected_idxs.append(np.argmax(word_doc_similarity))\n",
    "    selected_words.append(words[selected_idxs[0]])\n",
    "    remaining_idxs.remove(selected_idxs[0])\n",
    "    \n",
    "    for _ in range(top_n - 1):\n",
    "        mmr_scores = []\n",
    "        for idx in remaining_idxs:\n",
    "            sim_to_doc = word_doc_similarity[idx][0]\n",
    "            sim_to_selected = max([word_similarity[idx][j] for j in selected_idxs])\n",
    "            mmr_score = diversity * sim_to_doc - (1 - diversity) * sim_to_selected\n",
    "            mmr_scores.append((idx, mmr_score))\n",
    "        best_idx = sorted(mmr_scores, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        selected_idxs.append(best_idx)\n",
    "        selected_words.append(words[best_idx])\n",
    "        remaining_idxs.remove(best_idx)\n",
    "    return selected_words\n",
    "\n",
    "# 6. Ekstrak topik dan hitung MMR\n",
    "topics_keywords = {}\n",
    "for topic in topic_model.get_topics().keys():\n",
    "    if topic == -1:\n",
    "        continue\n",
    "    topic_words_scores = topic_model.get_topic(topic)\n",
    "    words = [w for w, _ in topic_words_scores]\n",
    "    embeddings_words = embedding_model.encode(words, show_progress_bar=False)\n",
    "    topic_embedding = np.mean(embedding_model.encode([\" \".join(words)]), axis=0)\n",
    "    top_keywords = mmr(topic_embedding, embeddings_words, words, top_n=10, diversity=0.7)\n",
    "    topics_keywords[topic] = top_keywords\n",
    "\n",
    "# 7. Simpan topik ke CSV\n",
    "topic_keywords_df = pd.DataFrame([\n",
    "    {\"Cluster\": k, \"Top_Keywords\": v}\n",
    "    for k, v in topics_keywords.items()\n",
    "])\n",
    "topic_keywords_df.to_csv(\"top_topic_keywords.csv\", index=False)\n",
    "\n",
    "# 8. Hitung dominant topic per game\n",
    "game_grouped = df.groupby(\"Game\")[\"cleaned_Reviews\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "results = []\n",
    "\n",
    "for _, row in game_grouped.iterrows():\n",
    "    game = row[\"Game\"]\n",
    "    review_text = row[\"cleaned_Reviews\"]\n",
    "\n",
    "    similarities = {}\n",
    "    for topic_id, keywords in topics_keywords.items():\n",
    "        topic_text = \" \".join(keywords)\n",
    "        try:\n",
    "            tfidf = vectorizer.fit([review_text, topic_text])\n",
    "            vecs = tfidf.transform([review_text, topic_text])\n",
    "            sim = cosine_similarity(vecs[0], vecs[1])[0][0]\n",
    "            similarities[topic_id] = sim\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if similarities:\n",
    "        dominant_topic = max(similarities, key=similarities.get)\n",
    "        similarity_score = similarities[dominant_topic]\n",
    "    else:\n",
    "        dominant_topic = -1\n",
    "        similarity_score = 0.0\n",
    "\n",
    "    results.append({\n",
    "        \"Game\": game,\n",
    "        \"Dominant_Topic\": dominant_topic,\n",
    "        \"Similarity_Score\": similarity_score\n",
    "    })\n",
    "\n",
    "# 9. Simpan ke CSV\n",
    "dominant_topic_df = pd.DataFrame(results)\n",
    "dominant_topic_df.to_csv(\"dominant_topic_per_game.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a216f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/divaoncom/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Coherence Score (c_v): 0.4047\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# 1. Ambil semua dokumen yang digunakan (harus sudah dalam bentuk list of tokens)\n",
    "tokenized_docs = [nltk.word_tokenize(doc.lower()) for doc in df['cleaned_Reviews']]\n",
    "\n",
    "# 2. Ambil topik dari BERTopic\n",
    "topics = topic_model.get_topics()\n",
    "topic_words = []\n",
    "for topic_id in sorted(topics.keys()):\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "    # Ambil 10 kata teratas untuk setiap topik\n",
    "    topic_keywords = [word for word, _ in topics[topic_id][:10]]\n",
    "    topic_words.append(topic_keywords)\n",
    "\n",
    "# 3. Buat dictionary dan corpus dari dokumen\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# 4. Hitung coherence\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topic_words,\n",
    "    texts=tokenized_docs,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'  # bisa juga 'u_mass', 'c_uci', 'c_npmi'\n",
    ")\n",
    "\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"✅ Coherence Score (c_v): {coherence_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d816b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/divaoncom/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/divaoncom/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Sekarang bisa download\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
