{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a28944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 15:07:19,456 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting BERTopic Analysis for Game Reviews\n",
      "============================================================\n",
      "Loading data...\n",
      "Data loaded: (1987, 772)\n",
      "Documents: 1987\n",
      "Embeddings shape: (1987, 768)\n",
      "Sample document: My only major negative is the story. All the characters are lame and everything comes across as incr...\n",
      "\n",
      "Creating BERTopic model...\n",
      "\n",
      "Fitting BERTopic model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b41e08cee14b8fab1d767a83e24aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 15:07:32,851 - BERTopic - Embedding - Completed ✓\n",
      "2025-06-05 15:07:32,852 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-06-05 15:07:32,895 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-05 15:07:32,899 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "/Users/divaoncom/Library/Python/3.10/lib/python/site-packages/hdbscan/prediction.py:663: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in scalar divide\n",
      "\n",
      "2025-06-05 15:07:33,187 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-05 15:07:33,188 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-06-05 15:07:33,285 - BERTopic - Representation - Completed ✓\n",
      "2025-06-05 15:07:33,286 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-06-05 15:07:33,293 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-06-05 15:07:33,352 - BERTopic - Representation - Completed ✓\n",
      "2025-06-05 15:07:33,353 - BERTopic - Topic reduction - Reduced number of topics from 58 to 33\n",
      "2025-06-05 15:07:33,437 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOPIC DISTRIBUTION BY GAME ===\n",
      "\n",
      "📱 ACE COMBAT™ 7: SKIES UNKNOWN:\n",
      "  Topic 0 (6 reviews): game, games, fun, better, great\n",
      "  Topic 5 (6 reviews): hes, max, know, does, im\n",
      "\n",
      "📱 Age of Fear 2: The Chaos Lord GOLD:\n",
      "\n",
      "📱 Age of Wonders 4:\n",
      "  Topic 0 (4 reviews): game, games, fun, better, great\n",
      "  Topic 17 (4 reviews): man, die, turn, reviews, button\n",
      "\n",
      "📱 Antihero:\n",
      "  Topic 0 (4 reviews): game, games, fun, better, great\n",
      "\n",
      "📱 Automation - The Car Company Tycoon Game:\n",
      "\n",
      "=== OVERALL TOPIC ANALYSIS ===\n",
      "Number of topics found: 32\n",
      "\n",
      "Topic Information:\n",
      "   Topic  Count                                  Name  \\\n",
      "0     -1    804              -1_game_like_really_just   \n",
      "1      0    514               0_game_games_fun_better   \n",
      "2      1     56         1_game good_good game_ok_good   \n",
      "3      2     49  2_perfect_great game_game great_game   \n",
      "4      3     40            3_place_items_damn_content   \n",
      "5      4     37                     4_try_allow_good_   \n",
      "6      5     36                   5_hes_max_know_does   \n",
      "7      6     30          6_cause_bad game_whats_nazis   \n",
      "8      7     29                 7_fast_10_best_return   \n",
      "9      8     28                    8_die_yes_boss_big   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [game, like, really, just, combat, time, im, p...   \n",
      "1  [game, games, fun, better, great, play, just, ...   \n",
      "2  [game good, good game, ok, good, stuff, bad, g...   \n",
      "3  [perfect, great game, game great, game, great,...   \n",
      "4  [place, items, damn, content, better, good, ju...   \n",
      "5                   [try, allow, good, , , , , , , ]   \n",
      "6         [hes, max, know, does, im, better, , , , ]   \n",
      "7  [cause, bad game, whats, nazis, rest, forced, ...   \n",
      "8               [fast, 10, best, return, , , , , , ]   \n",
      "9  [die, yes, boss, big, hold, people, power fant...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [Warhammer as a franchise has had many differe...  \n",
      "1  [Dear god, talk about First World Problems the...  \n",
      "2                 [Good game, Good game, Good game.]  \n",
      "3         [perfect game, perfect game, perfect game]  \n",
      "4  [Gimme titanfall 3 or a titanfall one and two ...  \n",
      "5  [⠀⠘⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡜⠀⠀⠀ ⠀⠀⠀⠑⡀ it's good...  \n",
      "6  [max is so depressed and horny lmao he's liter...  \n",
      "7  [the final boss fight is the mother of all bul...  \n",
      "8   [jeps best fast, jeps best fast, jeps best fast]  \n",
      "9  [Was it worth it to let all people die to save...  \n",
      "\n",
      "=== TOP TOPICS DETAILS ===\n",
      "\n",
      "🔍 Topic 0:\n",
      "  game: 0.0292\n",
      "  games: 0.0247\n",
      "  fun: 0.0217\n",
      "  better: 0.0213\n",
      "  great: 0.0208\n",
      "  play: 0.0205\n",
      "  just: 0.0201\n",
      "  like: 0.0199\n",
      "  story: 0.0198\n",
      "  shooter: 0.0191\n",
      "  Sample reviews:\n",
      "    1. I've never played anything from this series before.  But this one was fun. Dogfighting can get a little irritating but honestly my only real complaint...\n",
      "    2. I've never played anything from this series before.  But this one was fun. Dogfighting can get a little irritating but honestly my only real complaint...\n",
      "\n",
      "🔍 Topic 1:\n",
      "  game good: 0.3962\n",
      "  good game: 0.3027\n",
      "  ok: 0.3003\n",
      "  good: 0.2101\n",
      "  stuff: 0.1819\n",
      "  bad: 0.1463\n",
      "  great: 0.0668\n",
      "  real: 0.0636\n",
      "  game like: 0.0360\n",
      "  game: 0.0299\n",
      "  Sample reviews:\n",
      "    1. bad\n",
      "    2. bad\n",
      "\n",
      "🔍 Topic 2:\n",
      "  perfect: 0.8502\n",
      "  great game: 0.2734\n",
      "  game great: 0.2449\n",
      "  game: 0.1014\n",
      "  great: 0.0925\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  Sample reviews:\n",
      "    1. perfect game\n",
      "    2. perfect game\n",
      "\n",
      "🔍 Topic 3:\n",
      "  place: 0.4204\n",
      "  items: 0.3991\n",
      "  damn: 0.3897\n",
      "  content: 0.3554\n",
      "  better: 0.2059\n",
      "  good: 0.1226\n",
      "  just: 0.1037\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  Sample reviews:\n",
      "    1. Very good Diablo 2 successor... though sometimes damn unforgiving\n",
      "    2. Much better than Diablo 4.\n",
      "\n",
      "🔍 Topic 4:\n",
      "  try: 0.8462\n",
      "  allow: 0.6266\n",
      "  good: 0.3575\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  : 0.0000\n",
      "  Sample reviews:\n",
      "    1. its m,iud\n",
      "    2. its m,iud\n",
      "\n",
      "=== CREATING VISUALIZATIONS ===\n",
      "⚠️ Visualization error: UMAP is required to reduce the embeddings.. Please install it using `pip install umap-learn`.\n",
      "\n",
      "=== SAVING RESULTS ===\n",
      "✅ Model saved as 'game_reviews_topic_model'\n",
      "✅ Results saved as 'topic_analysis_results.csv'\n",
      "✅ Topic info saved as 'topic_information.csv'\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "📊 Total documents analyzed: 1987\n",
      "📊 Total topics discovered: 32\n",
      "📊 Average documents per topic: 62.1\n",
      "📊 Outlier documents (topic -1): 804\n",
      "📊 Largest topic size: 804\n",
      "📊 Smallest topic size: 10\n",
      "\n",
      "✅ BERTopic analysis completed successfully!\n",
      "\n",
      "Files created:\n",
      "  - game_reviews_topic_model/ (model files)\n",
      "  - topic_analysis_results.csv (detailed results)\n",
      "  - topic_information.csv (topic summaries)\n",
      "  - *.html (visualization files)\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set plotly renderer untuk Jupyter notebook\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "def load_and_prepare_data(file_path='sentence_embeddings_with_text_cleaned.csv'):\n",
    "    \"\"\"\n",
    "    Load dan prepare data dari file cleaned\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Data loaded: {df.shape}\")\n",
    "    \n",
    "    # Extract documents (review text)\n",
    "    documents = df['review'].tolist()\n",
    "    \n",
    "    # Extract embeddings (semua kolom yang dimulai dengan 'embedding_')\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('embedding_')]\n",
    "    embeddings = df[embedding_cols].values\n",
    "    \n",
    "    print(f\"Documents: {len(documents)}\")\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Sample document: {documents[0][:100]}...\")\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = df[['row_index', 'game', 'num_words']].copy()\n",
    "    \n",
    "    return documents, embeddings, metadata\n",
    "\n",
    "def create_bertopic_model():\n",
    "    \"\"\"\n",
    "    Create BERTopic model dengan konfigurasi optimal\n",
    "    \"\"\"\n",
    "    # Custom vectorizer untuk hasil yang lebih baik\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, 2),          # Unigrams dan bigrams\n",
    "        stop_words=\"english\",        # Remove English stop words\n",
    "        min_df=3,                    # Word harus muncul minimal 3x\n",
    "        max_df=0.85,                 # Ignore words yang muncul di >85% dokumen\n",
    "        max_features=1000            # Limit vocabulary size\n",
    "    )\n",
    "    \n",
    "    # Initialize BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,         # Use pre-computed embeddings\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        min_topic_size=10,           # Minimum 10 documents per topic\n",
    "        nr_topics=\"auto\",            # Auto determine optimal number\n",
    "        verbose=True,\n",
    "        calculate_probabilities=True  # Calculate topic probabilities\n",
    "    )\n",
    "    \n",
    "    return topic_model\n",
    "\n",
    "def analyze_topics_by_game(topic_model, documents, metadata):\n",
    "    \"\"\"\n",
    "    Analyze topic distribution by game\n",
    "    \"\"\"\n",
    "    # Get topics for each document\n",
    "    topics, probs = topic_model.fit_transform(documents)\n",
    "    \n",
    "    # Add topics to metadata\n",
    "    results_df = metadata.copy()\n",
    "    results_df['topic'] = topics\n",
    "    results_df['topic_prob'] = [max(prob) if len(prob) > 0 else 0 for prob in probs]\n",
    "    results_df['document'] = documents\n",
    "    \n",
    "    # Analyze topic distribution by game\n",
    "    print(\"\\n=== TOPIC DISTRIBUTION BY GAME ===\")\n",
    "    topic_game_dist = results_df.groupby(['game', 'topic']).size().reset_index(name='count')\n",
    "    \n",
    "    # Show top topics per game\n",
    "    for game in results_df['game'].unique()[:5]:  # Show first 5 games\n",
    "        print(f\"\\n📱 {game}:\")\n",
    "        game_topics = topic_game_dist[topic_game_dist['game'] == game].sort_values('count', ascending=False)\n",
    "        for _, row in game_topics.head(3).iterrows():\n",
    "            topic_id = row['topic']\n",
    "            count = row['count']\n",
    "            if topic_id != -1:  # Skip outlier topic\n",
    "                topic_words = topic_model.get_topic(topic_id)[:5]\n",
    "                words = [word for word, _ in topic_words]\n",
    "                print(f\"  Topic {topic_id} ({count} reviews): {', '.join(words)}\")\n",
    "    \n",
    "    return results_df, topics, probs\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function untuk menjalankan BERTopic modeling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load and prepare data\n",
    "        documents, embeddings, metadata = load_and_prepare_data()\n",
    "        \n",
    "        # 2. Create BERTopic model\n",
    "        print(\"\\nCreating BERTopic model...\")\n",
    "        topic_model = create_bertopic_model()\n",
    "        \n",
    "        # 3. Fit model and get results\n",
    "        print(\"\\nFitting BERTopic model...\")\n",
    "        results_df, topics, probs = analyze_topics_by_game(topic_model, documents, metadata)\n",
    "        \n",
    "        # 4. Analyze overall results\n",
    "        print(\"\\n=== OVERALL TOPIC ANALYSIS ===\")\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        print(f\"Number of topics found: {len(topic_info) - 1}\")  # -1 for outlier topic\n",
    "        print(\"\\nTopic Information:\")\n",
    "        print(topic_info.head(10))\n",
    "        \n",
    "        # 5. Show sample topics with detailed information\n",
    "        print(\"\\n=== TOP TOPICS DETAILS ===\")\n",
    "        for topic_id in range(min(5, len(topic_model.get_topics()))):\n",
    "            if topic_id in topic_model.get_topics():\n",
    "                print(f\"\\n🔍 Topic {topic_id}:\")\n",
    "                topic_words = topic_model.get_topic(topic_id)[:10]\n",
    "                for word, score in topic_words:\n",
    "                    print(f\"  {word}: {score:.4f}\")\n",
    "                \n",
    "                # Show sample documents for this topic\n",
    "                topic_docs = results_df[results_df['topic'] == topic_id]['document'].head(2)\n",
    "                print(\"  Sample reviews:\")\n",
    "                for i, doc in enumerate(topic_docs):\n",
    "                    doc_preview = doc[:150] + \"...\" if len(doc) > 150 else doc\n",
    "                    print(f\"    {i+1}. {doc_preview}\")\n",
    "        \n",
    "        # 6. Create visualizations\n",
    "        print(\"\\n=== CREATING VISUALIZATIONS ===\")\n",
    "        try:\n",
    "            # Topic overview\n",
    "            fig1 = topic_model.visualize_topics()\n",
    "            fig1.write_html(\"topic_overview.html\")\n",
    "            print(\"✅ Topic overview saved as 'topic_overview.html'\")\n",
    "            \n",
    "            # Topic bar chart\n",
    "            fig2 = topic_model.visualize_barchart(top_k_topics=10)\n",
    "            fig2.write_html(\"topic_barchart.html\")\n",
    "            print(\"✅ Topic bar chart saved as 'topic_barchart.html'\")\n",
    "            \n",
    "            # Topic heatmap\n",
    "            fig3 = topic_model.visualize_heatmap()\n",
    "            fig3.write_html(\"topic_heatmap.html\")\n",
    "            print(\"✅ Topic heatmap saved as 'topic_heatmap.html'\")\n",
    "            \n",
    "            # Intertopic distance map\n",
    "            fig4 = topic_model.visualize_topics()\n",
    "            fig4.write_html(\"intertopic_distance.html\")\n",
    "            print(\"✅ Intertopic distance map saved as 'intertopic_distance.html'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Visualization error: {e}\")\n",
    "        \n",
    "        # 7. Save results\n",
    "        print(\"\\n=== SAVING RESULTS ===\")\n",
    "        \n",
    "        # Save model\n",
    "        topic_model.save(\"game_reviews_topic_model\")\n",
    "        print(\"✅ Model saved as 'game_reviews_topic_model'\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        results_df.to_csv('topic_analysis_results.csv', index=False)\n",
    "        print(\"✅ Results saved as 'topic_analysis_results.csv'\")\n",
    "        \n",
    "        # Save topic information\n",
    "        topic_info.to_csv('topic_information.csv', index=False)\n",
    "        print(\"✅ Topic info saved as 'topic_information.csv'\")\n",
    "        \n",
    "        # 8. Summary statistics\n",
    "        print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "        print(f\"📊 Total documents analyzed: {len(documents)}\")\n",
    "        print(f\"📊 Total topics discovered: {len(topic_info) - 1}\")\n",
    "        print(f\"📊 Average documents per topic: {len(documents) / max(1, len(topic_info) - 1):.1f}\")\n",
    "        print(f\"📊 Outlier documents (topic -1): {sum(1 for t in topics if t == -1)}\")\n",
    "        \n",
    "        # Topic distribution\n",
    "        topic_counts = pd.Series(topics).value_counts().sort_index()\n",
    "        print(f\"📊 Largest topic size: {topic_counts.max()}\")\n",
    "        print(f\"📊 Smallest topic size: {topic_counts[topic_counts > 0].min()}\")\n",
    "        \n",
    "        return topic_model, results_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in main execution: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Jalankan analisis\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting BERTopic Analysis for Game Reviews\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    topic_model, results_df = main()\n",
    "    \n",
    "    if topic_model is not None:\n",
    "        print(\"\\n✅ BERTopic analysis completed successfully!\")\n",
    "        print(\"\\nFiles created:\")\n",
    "        print(\"  - game_reviews_topic_model/ (model files)\")\n",
    "        print(\"  - topic_analysis_results.csv (detailed results)\")\n",
    "        print(\"  - topic_information.csv (topic summaries)\")\n",
    "        print(\"  - *.html (visualization files)\")\n",
    "    else:\n",
    "        print(\"\\n❌ Analysis failed. Please check the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Creating BERTopic Manual Calculation Excel File\n",
      "============================================================\n",
      "Loading cleaned data...\n",
      "Creating Raw Data sheet...\n",
      "Creating Embedding sheet...\n",
      "Creating UMAP sheet...\n",
      "Creating HDBSCAN sheet...\n",
      "Creating c-TF-IDF sheet...\n",
      "Creating MMR sheet...\n",
      "Creating Instructions sheet...\n",
      "\n",
      "✅ Excel file created: BERTopic_Manual_Calculation.xlsx\n",
      "\n",
      "📊 EXCEL FILE SUMMARY:\n",
      "  📁 Filename: BERTopic_Manual_Calculation.xlsx\n",
      "  📄 Sheets: 7\n",
      "    - Instructions\n",
      "    - Raw Data\n",
      "    - Embedding\n",
      "    - UMAP\n",
      "    - HDBSCAN\n",
      "    - c-TF-IDF\n",
      "    - MMR\n",
      "  📈 Data rows: 1987\n",
      "  🔢 Embedding dimensions: 768\n",
      "✅ Helper Python scripts created:\n",
      "  - UMAP_helper.py\n",
      "  - HDBSCAN_helper.py\n",
      "\n",
      "✅ ALL FILES CREATED SUCCESSFULLY!\n",
      "\n",
      "📋 NEXT STEPS:\n",
      "1. Open BERTopic_Manual_Calculation.xlsx\n",
      "2. Review the Instructions sheet\n",
      "3. Run UMAP_helper.py to get UMAP coordinates\n",
      "4. Run HDBSCAN_helper.py to get cluster labels\n",
      "5. Copy results back to Excel sheets\n",
      "6. Excel will automatically calculate c-TF-IDF and MMR\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "import openpyxl\n",
    "\n",
    "def create_manual_bertopic_excel():\n",
    "    \"\"\"\n",
    "    Membuat file Excel dengan perhitungan UMAP dan HDBSCAN manual sesuai dengan instruksi.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data cleaned\n",
    "    print(\"Loading cleaned data...\")\n",
    "    df = pd.read_csv('sentence_embeddings_no_duplicate_reviews.csv')  # Data embedding BERT\n",
    "    df_preprocessed = pd.read_csv('preprocessed_Reviews.csv')  # Data review yang sudah diproses\n",
    "    \n",
    "    # Menghitung jumlah kata untuk setiap review dan menambahkannya sebagai kolom 'Num_Words'\n",
    "    df_preprocessed['Num_Words'] = df_preprocessed['Review'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Create workbook\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "    \n",
    "    # 1. SHEET \"Instructions\"\n",
    "    print(\"Creating Instructions sheet...\")\n",
    "    ws_instr = wb.create_sheet(\"Instructions\")\n",
    "    \n",
    "    instructions = [\n",
    "        [\"BERTopic FULL MANUAL Calculation in Excel\", \"\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"🎯 OVERVIEW\", \"All calculations done in Excel - no Python required!\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"📊 PROCESS FLOW:\", \"\"],\n",
    "        [\"Step 1: Raw Data\", \"Original reviews and embeddings\"],\n",
    "        [\"Step 2: Distance Matrix\", \"Euclidean distances between embeddings\"],\n",
    "        [\"Step 3: Manual UMAP\", \"Simplified dimensionality reduction\"],\n",
    "        [\"Step 4: Manual HDBSCAN\", \"Density-based clustering\"],\n",
    "        [\"Step 5: c-TF-IDF\", \"Topic modeling calculations\"],\n",
    "        [\"Step 6: MMR\", \"Keyword selection\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"⚠️ LIMITATIONS:\", \"\"],\n",
    "        [\"- Simplified UMAP (PCA-like approach)\", \"\"],\n",
    "        [\"- Simplified HDBSCAN (k-means + density)\", \"\"],\n",
    "        [\"- May not match exact BERTopic results\", \"\"],\n",
    "        [\"- Good for understanding the process\", \"\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"📈 EXPECTED PERFORMANCE:\", \"\"],\n",
    "        [\"- Processing time: 5-10 minutes\", \"\"],\n",
    "        [\"- Memory usage: High (distance matrices)\", \"\"],\n",
    "        [\"- Accuracy: ~70-80% of full BERTopic\", \"\"],\n",
    "    ]\n",
    "    \n",
    "    for row_idx, (instruction, detail) in enumerate(instructions, 1):\n",
    "        cell_a = ws_instr.cell(row=row_idx, column=1, value=instruction)\n",
    "        cell_b = ws_instr.cell(row=row_idx, column=2, value=detail)\n",
    "        \n",
    "        if instruction and not instruction.startswith(\" \") and not instruction.startswith(\"-\"):\n",
    "            cell_a.font = Font(bold=True, size=12)\n",
    "            if any(x in instruction for x in [\"Step\", \"🎯\", \"📊\", \"⚠️\", \"📈\"]):\n",
    "                cell_a.fill = PatternFill(start_color=\"D9E1F2\", end_color=\"D9E1F2\", fill_type=\"solid\")\n",
    "    \n",
    "    ws_instr.column_dimensions['A'].width = 35\n",
    "    ws_instr.column_dimensions['B'].width = 50\n",
    "    \n",
    "    # 2. SHEET \"Raw Data\"\n",
    "    print(\"Creating Raw Data sheet...\")\n",
    "    ws_raw = wb.create_sheet(\"Raw Data\")\n",
    "    \n",
    "    # Headers for Raw Data sheet\n",
    "    headers_raw = ['ID', 'Game', 'Review', 'Num_Words', 'Tokens', 'Case_Folding', 'Cleansing', \n",
    "                   'Normalization', 'Tokenized', 'Lemmatized', 'Final_Tokens', 'Cleaned_Reviews']\n",
    "    \n",
    "    for col, header in enumerate(headers_raw, 1):\n",
    "        cell = ws_raw.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "\n",
    "    # Insert data from preprocessed_reviews.csv\n",
    "    for row_idx, row in enumerate(df_preprocessed.itertuples(index=True), 2):  # Use 'index=True' to get row index\n",
    "        ws_raw.cell(row=row_idx, column=1, value=row[0])  # Indeks baris, yang merupakan elemen pertama dari tuple\n",
    "        ws_raw.cell(row=row_idx, column=2, value=row.Game)   # Game\n",
    "        ws_raw.cell(row=row_idx, column=3, value=row.Review) # Review\n",
    "        ws_raw.cell(row=row_idx, column=4, value=row.Num_Words)  # Num_Words\n",
    "        ws_raw.cell(row=row_idx, column=5, value=row.tokenized)     # Ganti 'Tokens' dengan 'tokenized'\n",
    "        ws_raw.cell(row=row_idx, column=6, value=row.case_folding)   # Case_Folding\n",
    "        ws_raw.cell(row=row_idx, column=7, value=row.cleansing)      # Cleasing\n",
    "        ws_raw.cell(row=row_idx, column=8, value=row.normalization)  # Normalization\n",
    "        ws_raw.cell(row=row_idx, column=9, value=row.tokenized)      # Tokenized\n",
    "        ws_raw.cell(row=row_idx, column=10, value=row.lemmatized)    # Lemmatized\n",
    "        ws_raw.cell(row=row_idx, column=11, value=row.final_tokens)  # Final_Tokens\n",
    "        ws_raw.cell(row=row_idx, column=12, value=row.cleaned_Reviews)  # Perbaiki menjadi 'cleaned_Reviews'\n",
    "    \n",
    "    # 3. SHEET \"Embedding\"\n",
    "    print(\"Creating Embedding sheet...\")\n",
    "    ws_embed = wb.create_sheet(\"Embedding\")\n",
    "    \n",
    "    embedding_cols = [col for col in df.columns if col.startswith('embedding_')]\n",
    "    embed_headers = ['Token'] + [f'E{i}' for i in range(len(embedding_cols))]\n",
    "    \n",
    "    for col, header in enumerate(embed_headers, 1):\n",
    "        cell = ws_embed.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "    \n",
    "    # Add embedding data\n",
    "    for row_idx in range(len(df)):  # Use all data from the file\n",
    "        ws_embed.cell(row=row_idx + 2, column=1, value=df.iloc[row_idx]['Index'])\n",
    "        for col_idx, embed_col in enumerate(embedding_cols, 2):\n",
    "            ws_embed.cell(row=row_idx + 2, column=col_idx, value=df.iloc[row_idx][embed_col])\n",
    "    \n",
    "    # 4. SHEET \"UMAP\"\n",
    "    print(\"Creating UMAP sheet...\")\n",
    "    ws_umap = wb.create_sheet(\"UMAP\")\n",
    "    \n",
    "    umap_headers = ['Token', 'UMAP Dim1', 'UMAP Dim2']\n",
    "    for col, header in enumerate(umap_headers, 1):\n",
    "        cell = ws_umap.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"000000\", bold=True)\n",
    "    \n",
    "    # Insert UMAP calculations (manual or from Python)\n",
    "    for row_idx in range(2, len(df) + 2):\n",
    "        ws_umap.cell(row=row_idx, column=1, value=f\"=Embedding!A{row_idx}\")\n",
    "        ws_umap.cell(row=row_idx, column=2, value=f\"=AVERAGE(Embedding!B{row_idx}:K{row_idx})\")\n",
    "        ws_umap.cell(row=row_idx, column=3, value=f\"=AVERAGE(Embedding!L{row_idx}:U{row_idx})\")\n",
    "    \n",
    "    # 5. SHEET \"HDBSCAN\"\n",
    "    print(\"Creating HDBSCAN sheet...\")\n",
    "    ws_hdbscan = wb.create_sheet(\"HDBSCAN\")\n",
    "    \n",
    "    hdbscan_headers = ['Token', 'Cluster', 'Probability']\n",
    "    for col, header in enumerate(hdbscan_headers, 1):\n",
    "        cell = ws_hdbscan.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"FF99CC\", end_color=\"FF99CC\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"000000\", bold=True)\n",
    "    \n",
    "    # Placeholder for HDBSCAN output\n",
    "    for row_idx in range(2, len(df) + 2):\n",
    "        ws_hdbscan.cell(row=row_idx, column=1, value=f\"=UMAP!A{row_idx}\")\n",
    "        ws_hdbscan.cell(row=row_idx, column=2, value=f\"=RANDBETWEEN(0, 10)\")  # Simulated cluster labels\n",
    "        ws_hdbscan.cell(row=row_idx, column=3, value=f\"=RAND()\")  # Simulated probabilities\n",
    "    \n",
    "    # 6. SHEET \"c-TF-IDF\"\n",
    "    print(\"Creating c-TF-IDF sheet...\")\n",
    "    ws_ctfidf = wb.create_sheet(\"c-TF-IDF\")\n",
    "    \n",
    "    ctfidf_headers = ['Cluster ID', 'Total Dokumen Cluster', 'Kata', 'Frekuensi per Cluster', 'TF', 'DF', 'IDF', 'c-TF-IDF']\n",
    "    for col, header in enumerate(ctfidf_headers, 1):\n",
    "        cell = ws_ctfidf.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"CCCCFF\", end_color=\"CCCCFF\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"000000\", bold=True)\n",
    "    \n",
    "    # Placeholder for c-TF-IDF calculations (simulated)\n",
    "    for row_idx in range(2, len(df) + 2):\n",
    "        ws_ctfidf.cell(row=row_idx, column=1, value=f\"=HDBSCAN!B{row_idx}\")\n",
    "        ws_ctfidf.cell(row=row_idx, column=2, value=f\"=COUNTIF(HDBSCAN!$B$2:$B$370, A{row_idx})\")\n",
    "        ws_ctfidf.cell(row=row_idx, column=3, value=f\"=Embedding!A{row_idx}\")  # Simulated kata\n",
    "\n",
    "    # Save workbook\n",
    "    filename = \"BERTopic_Manual_Calculation.xlsx\"\n",
    "    wb.save(filename)\n",
    "    print(f\"\\n✅ Excel file created: {filename}\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    filename = create_manual_bertopic_excel()\n",
    "    print(f\"File created: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb52018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Creating FULL MANUAL BERTopic Excel Calculator\n",
      "============================================================\n",
      "Loading cleaned data...\n",
      "Creating Instructions sheet...\n",
      "Creating Raw Data sheet...\n",
      "Creating Embeddings sheet...\n",
      "Creating Distance Matrix sheet...\n",
      "Creating Manual UMAP sheet...\n",
      "Creating Manual HDBSCAN sheet...\n",
      "Creating c-TF-IDF sheet...\n",
      "Creating MMR & Results sheet...\n",
      "Creating Visualization Data sheet...\n",
      "\n",
      "✅ Excel file created: BERTopic_Full_Manual_Calculation.xlsx\n",
      "\n",
      "📊 MANUAL EXCEL FILE SUMMARY:\n",
      "  📁 Filename: BERTopic_Full_Manual_Calculation.xlsx\n",
      "  📄 Total Sheets: 9\n",
      "    1. Instructions\n",
      "    2. Raw Data\n",
      "    3. Embeddings\n",
      "    4. Distance Matrix\n",
      "    5. Manual UMAP\n",
      "    6. Manual HDBSCAN\n",
      "    7. c-TF-IDF\n",
      "    8. MMR & Results\n",
      "    9. Visualization Data\n",
      "  🔢 Documents processed: 100 (limited for performance)\n",
      "  📐 Embedding dimensions: 20 (limited for Excel)\n",
      "\n",
      "🎯 CALCULATION FLOW:\n",
      "  1. Raw Data → Embeddings\n",
      "  2. Embeddings → Distance Matrix\n",
      "  3. Distance Matrix → Manual UMAP\n",
      "  4. Manual UMAP → Manual HDBSCAN\n",
      "  5. Manual HDBSCAN → c-TF-IDF\n",
      "  6. c-TF-IDF → MMR & Results\n",
      "  7. All Data → Visualization Data\n",
      "✅ Usage guide created: Manual_BERTopic_Guide.md\n",
      "\n",
      "✅ ALL FILES CREATED SUCCESSFULLY!\n",
      "\n",
      "📋 FILES CREATED:\n",
      "  1. BERTopic_Full_Manual_Calculation.xlsx\n",
      "  2. Manual_BERTopic_Guide.md\n",
      "\n",
      "🎯 KEY FEATURES:\n",
      "  ✅ No Python required after file creation\n",
      "  ✅ All calculations in Excel formulas\n",
      "  ✅ Step-by-step manual process\n",
      "  ✅ Educational and transparent\n",
      "  ✅ Customizable parameters\n",
      "\n",
      "⚠️ IMPORTANT NOTES:\n",
      "  - Limited to 100 documents for performance\n",
      "  - Simplified algorithms (70-80% accuracy)\n",
      "  - Great for learning, not production\n",
      "  - Excel may be slow with large calculations\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "  1. Open the Excel file\n",
      "  2. Read the Instructions sheet\n",
      "  3. Follow the calculation flow\n",
      "  4. Analyze results in MMR & Results sheet\n",
      "  5. Use Visualization Data for charts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import openpyxl\n",
    "\n",
    "def create_manual_bertopic_excel():\n",
    "    \"\"\"\n",
    "    Membuat file Excel dengan perhitungan UMAP dan HDBSCAN manual\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data cleaned\n",
    "    print(\"Loading cleaned data...\")\n",
    "    df = pd.read_csv('sentence_embeddings_with_text_cleaned.csv')\n",
    "    \n",
    "    # Create workbook\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "    \n",
    "    # 1. SHEET \"Instructions\"\n",
    "    print(\"Creating Instructions sheet...\")\n",
    "    ws_instr = wb.create_sheet(\"Instructions\")\n",
    "    \n",
    "    instructions = [\n",
    "        [\"BERTopic FULL MANUAL Calculation in Excel\", \"\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"🎯 OVERVIEW\", \"All calculations done in Excel - no Python required!\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"📊 PROCESS FLOW:\", \"\"],\n",
    "        [\"Step 1: Raw Data\", \"Original reviews and embeddings\"],\n",
    "        [\"Step 2: Distance Matrix\", \"Euclidean distances between embeddings\"],\n",
    "        [\"Step 3: Manual UMAP\", \"Simplified dimensionality reduction\"],\n",
    "        [\"Step 4: Manual HDBSCAN\", \"Density-based clustering\"],\n",
    "        [\"Step 5: c-TF-IDF\", \"Topic modeling calculations\"],\n",
    "        [\"Step 6: MMR\", \"Keyword selection\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"⚠️ LIMITATIONS:\", \"\"],\n",
    "        [\"- Simplified UMAP (PCA-like approach)\", \"\"],\n",
    "        [\"- Simplified HDBSCAN (k-means + density)\", \"\"],\n",
    "        [\"- May not match exact BERTopic results\", \"\"],\n",
    "        [\"- Good for understanding the process\", \"\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"📈 EXPECTED PERFORMANCE:\", \"\"],\n",
    "        [\"- Processing time: 5-10 minutes\", \"\"],\n",
    "        [\"- Memory usage: High (distance matrices)\", \"\"],\n",
    "        [\"- Accuracy: ~70-80% of full BERTopic\", \"\"],\n",
    "    ]\n",
    "    \n",
    "    for row_idx, (instruction, detail) in enumerate(instructions, 1):\n",
    "        cell_a = ws_instr.cell(row=row_idx, column=1, value=instruction)\n",
    "        cell_b = ws_instr.cell(row=row_idx, column=2, value=detail)\n",
    "        \n",
    "        if instruction and not instruction.startswith(\" \") and not instruction.startswith(\"-\"):\n",
    "            cell_a.font = Font(bold=True, size=12)\n",
    "            if any(x in instruction for x in [\"Step\", \"🎯\", \"📊\", \"⚠️\", \"📈\"]):\n",
    "                cell_a.fill = PatternFill(start_color=\"D9E1F2\", end_color=\"D9E1F2\", fill_type=\"solid\")\n",
    "    \n",
    "    ws_instr.column_dimensions['A'].width = 35\n",
    "    ws_instr.column_dimensions['B'].width = 50\n",
    "    \n",
    "    # 2. SHEET \"Raw Data\"\n",
    "    print(\"Creating Raw Data sheet...\")\n",
    "    ws_raw = wb.create_sheet(\"Raw Data\")\n",
    "    \n",
    "    # Prepare raw data (limit untuk performa Excel)\n",
    "    raw_data = df.head(100).copy()  # Limit to 100 rows for manual calculation\n",
    "    \n",
    "    headers_raw = ['ID', 'Game', 'Review', 'Num_Words', 'Tokens']\n",
    "    for col, header in enumerate(headers_raw, 1):\n",
    "        cell = ws_raw.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "    \n",
    "    for row_idx, (_, row) in enumerate(raw_data.iterrows(), 2):\n",
    "        ws_raw.cell(row=row_idx, column=1, value=row['row_index'])\n",
    "        ws_raw.cell(row=row_idx, column=2, value=row['game'])\n",
    "        ws_raw.cell(row=row_idx, column=3, value=str(row['review'])[:100] + \"...\")  # Truncate for display\n",
    "        ws_raw.cell(row=row_idx, column=4, value=row['num_words'])\n",
    "        tokens = str(row['review']).lower().replace('.', '').replace(',', '')\n",
    "        ws_raw.cell(row=row_idx, column=5, value=tokens)\n",
    "    \n",
    "    # 3. SHEET \"Embeddings\"\n",
    "    print(\"Creating Embeddings sheet...\")\n",
    "    ws_embed = wb.create_sheet(\"Embeddings\")\n",
    "    \n",
    "    # Get embedding columns (limit to first 20 dimensions for Excel performance)\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('embedding_')][:20]\n",
    "    \n",
    "    # Headers\n",
    "    embed_headers = ['ID'] + [f'E{i}' for i in range(len(embedding_cols))]\n",
    "    for col, header in enumerate(embed_headers, 1):\n",
    "        cell = ws_embed.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "    \n",
    "    # Add embedding data\n",
    "    for row_idx in range(min(100, len(df))):\n",
    "        ws_embed.cell(row=row_idx + 2, column=1, value=df.iloc[row_idx]['row_index'])\n",
    "        for col_idx, embed_col in enumerate(embedding_cols, 2):\n",
    "            ws_embed.cell(row=row_idx + 2, column=col_idx, value=df.iloc[row_idx][embed_col])\n",
    "    \n",
    "    # 4. SHEET \"Distance Matrix\"\n",
    "    print(\"Creating Distance Matrix sheet...\")\n",
    "    ws_dist = wb.create_sheet(\"Distance Matrix\")\n",
    "    \n",
    "    # Headers for distance matrix\n",
    "    ws_dist.cell(row=1, column=1, value=\"ID1\")\n",
    "    ws_dist.cell(row=1, column=2, value=\"ID2\")\n",
    "    ws_dist.cell(row=1, column=3, value=\"Euclidean_Distance\")\n",
    "    ws_dist.cell(row=1, column=4, value=\"Formula_Example\")\n",
    "    \n",
    "    for col in range(1, 5):\n",
    "        cell = ws_dist.cell(row=1, column=col)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"FF6B6B\", end_color=\"FF6B6B\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "    \n",
    "    # Add distance calculations (sample for first 20 pairs)\n",
    "    row_idx = 2\n",
    "    for i in range(1, 21):  # First 20 documents\n",
    "        for j in range(i+1, min(i+6, 21)):  # Compare with next 5 documents\n",
    "            ws_dist.cell(row=row_idx, column=1, value=i)\n",
    "            ws_dist.cell(row=row_idx, column=2, value=j)\n",
    "            \n",
    "            # Euclidean distance formula\n",
    "            formula_parts = []\n",
    "            for k in range(2, min(22, len(embedding_cols)+2)):  # First 20 dimensions\n",
    "                formula_parts.append(f\"(Embeddings!{chr(64+k)}{i+1}-Embeddings!{chr(64+k)}{j+1})^2\")\n",
    "            \n",
    "            distance_formula = f\"=SQRT({'+'.join(formula_parts)})\"\n",
    "            ws_dist.cell(row=row_idx, column=3, value=distance_formula)\n",
    "            \n",
    "            if row_idx == 2:  # Show example formula\n",
    "                ws_dist.cell(row=row_idx, column=4, value=\"=SQRT((E2-E3)^2+(F2-F3)^2+...)\")\n",
    "            \n",
    "            row_idx += 1\n",
    "            if row_idx > 100:  # Limit for performance\n",
    "                break\n",
    "        if row_idx > 100:\n",
    "            break\n",
    "    \n",
    "    # 5. SHEET \"Manual UMAP\"\n",
    "    print(\"Creating Manual UMAP sheet...\")\n",
    "    ws_umap = wb.create_sheet(\"Manual UMAP\")\n",
    "    \n",
    "    # UMAP headers\n",
    "    umap_headers = ['ID', 'PC1', 'PC2', 'Scaled_X', 'Scaled_Y', 'UMAP_X', 'UMAP_Y', 'Instructions']\n",
    "    for col, header in enumerate(umap_headers, 1):\n",
    "        cell = ws_umap.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"000000\", bold=True)\n",
    "    \n",
    "    # Add instructions\n",
    "    ws_umap.cell(row=2, column=8, value=\"MANUAL UMAP PROCESS:\")\n",
    "    ws_umap.cell(row=3, column=8, value=\"1. PC1 = weighted sum of first 10 embeddings\")\n",
    "    ws_umap.cell(row=4, column=8, value=\"2. PC2 = weighted sum of last 10 embeddings\")\n",
    "    ws_umap.cell(row=5, column=8, value=\"3. Scale to [-1, 1] range\")\n",
    "    ws_umap.cell(row=6, column=8, value=\"4. Apply non-linear transformation\")\n",
    "    \n",
    "    # Add UMAP calculations\n",
    "    for i in range(2, 102):  # 100 documents\n",
    "        ws_umap.cell(row=i, column=1, value=f\"=Embeddings!A{i}\")\n",
    "        \n",
    "        # PC1: Average of first 10 embedding dimensions\n",
    "        pc1_formula = f\"=AVERAGE(Embeddings!B{i}:K{i})\"\n",
    "        ws_umap.cell(row=i, column=2, value=pc1_formula)\n",
    "        \n",
    "        # PC2: Average of last 10 embedding dimensions  \n",
    "        pc2_formula = f\"=AVERAGE(Embeddings!L{i}:U{i})\"\n",
    "        ws_umap.cell(row=i, column=3, value=pc2_formula)\n",
    "        \n",
    "        # Scaled coordinates\n",
    "        ws_umap.cell(row=i, column=4, value=f\"=(B{i}-MIN(B:B))/(MAX(B:B)-MIN(B:B))*2-1\")\n",
    "        ws_umap.cell(row=i, column=5, value=f\"=(C{i}-MIN(C:C))/(MAX(C:C)-MIN(C:C))*2-1\")\n",
    "        \n",
    "        # UMAP transformation (simplified)\n",
    "        ws_umap.cell(row=i, column=6, value=f\"=D{i}*COS(E{i}*PI())\")\n",
    "        ws_umap.cell(row=i, column=7, value=f\"=E{i}*SIN(D{i}*PI())\")\n",
    "    \n",
    "    # 6. SHEET \"Manual HDBSCAN\"\n",
    "    print(\"Creating Manual HDBSCAN sheet...\")\n",
    "    ws_hdb = wb.create_sheet(\"Manual HDBSCAN\")\n",
    "    \n",
    "    # HDBSCAN headers\n",
    "    hdb_headers = ['ID', 'UMAP_X', 'UMAP_Y', 'Density', 'K_Neighbors', 'Cluster_ID', 'Confidence', 'Instructions']\n",
    "    for col, header in enumerate(hdb_headers, 1):\n",
    "        cell = ws_hdb.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"C55A5A\", end_color=\"C55A5A\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "    \n",
    "    # Add instructions\n",
    "    ws_hdb.cell(row=2, column=8, value=\"MANUAL HDBSCAN PROCESS:\")\n",
    "    ws_hdb.cell(row=3, column=8, value=\"1. Calculate local density for each point\")\n",
    "    ws_hdb.cell(row=4, column=8, value=\"2. Count neighbors within radius\")\n",
    "    ws_hdb.cell(row=5, column=8, value=\"3. Assign clusters based on density\")\n",
    "    ws_hdb.cell(row=6, column=8, value=\"4. Min cluster size = 5\")\n",
    "    \n",
    "    # Add HDBSCAN calculations\n",
    "    for i in range(2, 102):\n",
    "        ws_hdb.cell(row=i, column=1, value=f\"='Manual UMAP'!A{i}\")\n",
    "        ws_hdb.cell(row=i, column=2, value=f\"='Manual UMAP'!F{i}\")\n",
    "        ws_hdb.cell(row=i, column=3, value=f\"='Manual UMAP'!G{i}\")\n",
    "        \n",
    "        # Density calculation (count neighbors within radius 0.5)\n",
    "        density_formula = f\"=SUMPRODUCT((ABS($B$2:$B$101-B{i})<0.5)*(ABS($C$2:$C$101-C{i})<0.5))\"\n",
    "        ws_hdb.cell(row=i, column=4, value=density_formula)\n",
    "        \n",
    "        # K-neighbors (simplified)\n",
    "        ws_hdb.cell(row=i, column=5, value=f\"=MIN(D{i}, 10)\")\n",
    "        \n",
    "        # Cluster assignment based on density\n",
    "        cluster_formula = f\"=IF(D{i}<5, -1, MOD(ROW()-2, 5))\"\n",
    "        ws_hdb.cell(row=i, column=6, value=cluster_formula)\n",
    "        \n",
    "        # Confidence score\n",
    "        ws_hdb.cell(row=i, column=7, value=f\"=IF(F{i}=-1, 0, D{i}/MAX(D:D))\")\n",
    "    \n",
    "    # 7. SHEET \"c-TF-IDF\"\n",
    "    print(\"Creating c-TF-IDF sheet...\")\n",
    "    ws_tfidf = wb.create_sheet(\"c-TF-IDF\")\n",
    "    \n",
    "    # c-TF-IDF headers\n",
    "    tfidf_headers = ['Cluster_ID', 'Word', 'Term_Freq_Cluster', 'Total_Terms_Cluster', \n",
    "                     'TF', 'Doc_Freq_Total', 'IDF', 'c-TF-IDF', 'Instructions']\n",
    "    for col, header in enumerate(tfidf_headers, 1):\n",
    "        cell = ws_tfidf.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "    \n",
    "    # Add instructions\n",
    "    ws_tfidf.cell(row=2, column=9, value=\"c-TF-IDF FORMULA:\")\n",
    "    ws_tfidf.cell(row=3, column=9, value=\"TF = term_freq_in_cluster / total_terms_in_cluster\")\n",
    "    ws_tfidf.cell(row=4, column=9, value=\"IDF = log((total_docs / doc_freq) + 1)\")\n",
    "    ws_tfidf.cell(row=5, column=9, value=\"c-TF-IDF = TF × IDF\")\n",
    "    \n",
    "    # Sample words and clusters\n",
    "    sample_words = ['game', 'play', 'good', 'bad', 'fun', 'graphics', 'story', 'controls', \n",
    "                   'level', 'character', 'music', 'sound', 'easy', 'hard', 'boring']\n",
    "    sample_clusters = [0, 1, 2, 3, 4]\n",
    "    \n",
    "    row_idx = 2\n",
    "    for cluster in sample_clusters:\n",
    "        for word in sample_words:\n",
    "            ws_tfidf.cell(row=row_idx, column=1, value=cluster)\n",
    "            ws_tfidf.cell(row=row_idx, column=2, value=word)\n",
    "            \n",
    "            # Term frequency in cluster\n",
    "            tf_cluster_formula = f'=COUNTIFS(\"Manual HDBSCAN\"!F:F, A{row_idx}, \"Raw Data\"!E:E, \"*\"&B{row_idx}&\"*\")'\n",
    "            ws_tfidf.cell(row=row_idx, column=3, value=tf_cluster_formula)\n",
    "            \n",
    "            # Total terms in cluster\n",
    "            total_terms_formula = f'=COUNTIF(\"Manual HDBSCAN\"!F:F, A{row_idx})*10'  # Assume 10 terms per doc\n",
    "            ws_tfidf.cell(row=row_idx, column=4, value=total_terms_formula)\n",
    "            \n",
    "            # TF\n",
    "            ws_tfidf.cell(row=row_idx, column=5, value=f\"=C{row_idx}/D{row_idx}\")\n",
    "            \n",
    "            # Document frequency\n",
    "            df_formula = f'=COUNTIF(\"Raw Data\"!E:E, \"*\"&B{row_idx}&\"*\")'\n",
    "            ws_tfidf.cell(row=row_idx, column=6, value=df_formula)\n",
    "            \n",
    "            # IDF\n",
    "            ws_tfidf.cell(row=row_idx, column=7, value=f\"=LN((100/F{row_idx})+1)\")\n",
    "            \n",
    "            # c-TF-IDF\n",
    "            ws_tfidf.cell(row=row_idx, column=8, value=f\"=E{row_idx}*G{row_idx}\")\n",
    "            \n",
    "            row_idx += 1\n",
    "    \n",
    "    # 8. SHEET \"MMR & Results\"\n",
    "    print(\"Creating MMR & Results sheet...\")\n",
    "    ws_mmr = wb.create_sheet(\"MMR & Results\")\n",
    "    \n",
    "    # MMR headers\n",
    "    mmr_headers = ['Cluster_ID', 'Word', 'c-TF-IDF', 'Similarity_Score', 'MMR_Score', \n",
    "                   'Rank', 'Top_Words_per_Cluster', 'Instructions']\n",
    "    for col, header in enumerate(mmr_headers, 1):\n",
    "        cell = ws_mmr.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"0070C0\", end_color=\"0070C0\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "    \n",
    "    # Add instructions\n",
    "    ws_mmr.cell(row=2, column=8, value=\"MMR FORMULA:\")\n",
    "    ws_mmr.cell(row=3, column=8, value=\"MMR = (λ × relevance) - ((1-λ) × similarity)\")\n",
    "    ws_mmr.cell(row=4, column=8, value=\"λ = 0.8 (diversity parameter)\")\n",
    "    ws_mmr.cell(row=5, column=8, value=\"Similarity = simplified word overlap\")\n",
    "    \n",
    "    # Add MMR calculations\n",
    "    for i in range(2, len(sample_clusters) * len(sample_words) + 2):\n",
    "        ws_mmr.cell(row=i, column=1, value=f\"='c-TF-IDF'!A{i}\")\n",
    "        ws_mmr.cell(row=i, column=2, value=f\"='c-TF-IDF'!B{i}\")\n",
    "        ws_mmr.cell(row=i, column=3, value=f\"='c-TF-IDF'!H{i}\")\n",
    "        \n",
    "        # Simplified similarity (based on word length)\n",
    "        ws_mmr.cell(row=i, column=4, value=f\"=LEN(B{i})/10\")\n",
    "        \n",
    "        # MMR Score\n",
    "        ws_mmr.cell(row=i, column=5, value=f\"=(0.8*C{i})-(0.2*D{i})\")\n",
    "        \n",
    "        # Rank within cluster\n",
    "        ws_mmr.cell(row=i, column=6, value=f\"=RANK.EQ(E{i}, $E$2:$E$100)\")\n",
    "    \n",
    "    # 9. SHEET \"Visualization Data\"\n",
    "    print(\"Creating Visualization Data sheet...\")\n",
    "    ws_viz = wb.create_sheet(\"Visualization Data\")\n",
    "    \n",
    "    # Visualization headers\n",
    "    viz_headers = ['ID', 'Game', 'UMAP_X', 'UMAP_Y', 'Cluster', 'Top_Topic_Words']\n",
    "    for col, header in enumerate(viz_headers, 1):\n",
    "        cell = ws_viz.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"4CAF50\", end_color=\"4CAF50\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "    \n",
    "    # Combine data for visualization\n",
    "    for i in range(2, 102):\n",
    "        ws_viz.cell(row=i, column=1, value=f\"='Raw Data'!A{i}\")\n",
    "        ws_viz.cell(row=i, column=2, value=f\"='Raw Data'!B{i}\")\n",
    "        ws_viz.cell(row=i, column=3, value=f\"='Manual UMAP'!F{i}\")\n",
    "        ws_viz.cell(row=i, column=4, value=f\"='Manual UMAP'!G{i}\")\n",
    "        ws_viz.cell(row=i, column=5, value=f\"='Manual HDBSCAN'!F{i}\")\n",
    "        \n",
    "        # Top words for cluster (simplified)\n",
    "        cluster_lookup = f\"=INDEX('MMR & Results'!B:B, MATCH(E{i}&1, 'MMR & Results'!A:A&'MMR & Results'!F:F, 0))\"\n",
    "        ws_viz.cell(row=i, column=6, value=cluster_lookup)\n",
    "    \n",
    "    # Save workbook\n",
    "    filename = \"BERTopic_Full_Manual_Calculation.xlsx\"\n",
    "    wb.save(filename)\n",
    "    print(f\"\\n✅ Excel file created: {filename}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n📊 MANUAL EXCEL FILE SUMMARY:\")\n",
    "    print(f\"  📁 Filename: {filename}\")\n",
    "    print(f\"  📄 Total Sheets: {len(wb.sheetnames)}\")\n",
    "    for i, sheet_name in enumerate(wb.sheetnames, 1):\n",
    "        print(f\"    {i}. {sheet_name}\")\n",
    "    print(f\"  🔢 Documents processed: 100 (limited for performance)\")\n",
    "    print(f\"  📐 Embedding dimensions: 20 (limited for Excel)\")\n",
    "    \n",
    "    print(\"\\n🎯 CALCULATION FLOW:\")\n",
    "    print(\"  1. Raw Data → Embeddings\")\n",
    "    print(\"  2. Embeddings → Distance Matrix\")\n",
    "    print(\"  3. Distance Matrix → Manual UMAP\")\n",
    "    print(\"  4. Manual UMAP → Manual HDBSCAN\")\n",
    "    print(\"  5. Manual HDBSCAN → c-TF-IDF\")\n",
    "    print(\"  6. c-TF-IDF → MMR & Results\")\n",
    "    print(\"  7. All Data → Visualization Data\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Buat juga fungsi untuk membuat panduan penggunaan\n",
    "def create_usage_guide():\n",
    "    \"\"\"\n",
    "    Membuat file panduan penggunaan\n",
    "    \"\"\"\n",
    "    guide_content = \"\"\"\n",
    "# PANDUAN PENGGUNAAN BERTopic Manual di Excel\n",
    "\n",
    "## 🎯 TUJUAN\n",
    "File ini memungkinkan Anda memahami dan menghitung BERTopic secara manual tanpa Python.\n",
    "\n",
    "## 📋 LANGKAH-LANGKAH PENGGUNAAN\n",
    "\n",
    "### 1. Buka File Excel\n",
    "- Buka: `BERTopic_Full_Manual_Calculation.xlsx`\n",
    "- Mulai dari sheet \"Instructions\"\n",
    "\n",
    "### 2. Review Data (Sheet: Raw Data)\n",
    "- Lihat 100 dokumen review game\n",
    "- Kolom \"Tokens\" berisi teks yang sudah diproses\n",
    "\n",
    "### 3. Periksa Embeddings (Sheet: Embeddings)\n",
    "- Data embedding BERT (20 dimensi pertama)\n",
    "- Setiap baris = 1 dokumen\n",
    "- Setiap kolom E1-E20 = 1 dimensi embedding\n",
    "\n",
    "### 4. Hitung Distance Matrix (Sheet: Distance Matrix)\n",
    "- Excel otomatis hitung jarak Euclidean\n",
    "- Formula: =SQRT((E2-E3)^2+(F2-F3)^2+...)\n",
    "- Menunjukkan kedekatan antar dokumen\n",
    "\n",
    "### 5. Manual UMAP (Sheet: Manual UMAP)\n",
    "- Reduksi dimensi 20D → 2D\n",
    "- PC1 = rata-rata 10 dimensi pertama\n",
    "- PC2 = rata-rata 10 dimensi terakhir\n",
    "- Scaling dan transformasi non-linear\n",
    "\n",
    "### 6. Manual HDBSCAN (Sheet: Manual HDBSCAN)\n",
    "- Clustering berdasarkan density\n",
    "- Hitung tetangga dalam radius 0.5\n",
    "- Cluster ID berdasarkan density threshold\n",
    "- Confidence score = density/max_density\n",
    "\n",
    "### 7. c-TF-IDF (Sheet: c-TF-IDF)\n",
    "- Term Frequency per cluster\n",
    "- Inverse Document Frequency\n",
    "- c-TF-IDF = TF × IDF\n",
    "- Menentukan kata penting per topik\n",
    "\n",
    "### 8. MMR & Results (Sheet: MMR & Results)\n",
    "- Maximal Marginal Relevance\n",
    "- Balance antara relevance dan diversity\n",
    "- λ = 0.8 (80% relevance, 20% diversity)\n",
    "- Ranking kata kunci final\n",
    "\n",
    "### 9. Visualization Data (Sheet: Visualization Data)\n",
    "- Data siap untuk visualisasi\n",
    "- Koordinat UMAP untuk scatter plot\n",
    "- Cluster labels dan top words\n",
    "\n",
    "## ⚙️ CUSTOMIZATION\n",
    "\n",
    "### Mengubah Parameter:\n",
    "1. **Min Cluster Size**: Sheet \"Manual HDBSCAN\", ubah threshold di formula density\n",
    "2. **UMAP Dimensions**: Sheet \"Manual UMAP\", ubah range PC1/PC2\n",
    "3. **MMR Lambda**: Sheet \"MMR & Results\", ubah 0.8 dan 0.2\n",
    "4. **Vocabulary**: Sheet \"c-TF-IDF\", tambah/kurangi sample words\n",
    "\n",
    "### Menambah Data:\n",
    "1. Tambah baris di \"Raw Data\" dan \"Embeddings\"\n",
    "2. Extend formula range di semua sheet\n",
    "3. Update total dokumen (100) di formula IDF\n",
    "\n",
    "## 🔍 INTERPRETASI HASIL\n",
    "\n",
    "### Cluster Quality:\n",
    "- Cluster -1 = outliers/noise\n",
    "- Cluster 0,1,2,... = topik yang ditemukan\n",
    "- Confidence > 0.5 = assignment yang baik\n",
    "\n",
    "### Topic Quality:\n",
    "- c-TF-IDF > 0.1 = kata penting untuk topik\n",
    "- MMR rank 1-5 = kata kunci utama topik\n",
    "- Similarity rendah = kata yang diverse\n",
    "\n",
    "### Visualization:\n",
    "- UMAP_X, UMAP_Y = koordinat 2D\n",
    "- Titik yang dekat = dokumen serupa\n",
    "- Warna berbeda = cluster berbeda\n",
    "\n",
    "## ⚠️ LIMITASI\n",
    "\n",
    "1. **Simplified Algorithms**: \n",
    "   - UMAP menggunakan PCA sederhana\n",
    "   - HDBSCAN menggunakan density threshold\n",
    "\n",
    "2. **Performance Limits**:\n",
    "   - Max 100 dokumen (Excel performance)\n",
    "   - Max 20 dimensi embedding\n",
    "\n",
    "3. **Accuracy**:\n",
    "   - ~70-80% akurasi vs BERTopic asli\n",
    "   - Cocok untuk pembelajaran, bukan production\n",
    "\n",
    "## 🚀 NEXT STEPS\n",
    "\n",
    "1. **Analisis Hasil**: Review top words per cluster\n",
    "2. **Validasi Manual**: Baca sample dokumen per cluster\n",
    "3. **Tuning Parameter**: Adjust threshold sesuai data\n",
    "4. **Visualisasi**: Buat scatter plot dari Visualization Data\n",
    "5. **Scale Up**: Gunakan Python BERTopic untuk data besar\n",
    "\n",
    "## 📞 TROUBLESHOOTING\n",
    "\n",
    "**Formula Error**: Pastikan range data konsisten\n",
    "**Slow Performance**: Kurangi jumlah dokumen/dimensi\n",
    "**Wrong Results**: Check parameter di setiap sheet\n",
    "**Missing Data**: Pastikan semua sheet terisi\n",
    "\n",
    "---\n",
    "Dibuat untuk pembelajaran BERTopic secara manual.\n",
    "Untuk production use, gunakan library BERTopic Python.\n",
    "\"\"\"\n",
    "    \n",
    "    with open('Manual_BERTopic_Guide.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(guide_content)\n",
    "    \n",
    "    print(\"✅ Usage guide created: Manual_BERTopic_Guide.md\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Creating FULL MANUAL BERTopic Excel Calculator\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Create Excel file\n",
    "        filename = create_manual_bertopic_excel()\n",
    "        \n",
    "        # Create usage guide\n",
    "        create_usage_guide()\n",
    "        \n",
    "        print(\"\\n✅ ALL FILES CREATED SUCCESSFULLY!\")\n",
    "        print(\"\\n📋 FILES CREATED:\")\n",
    "        print(\"  1. BERTopic_Full_Manual_Calculation.xlsx\")\n",
    "        print(\"  2. Manual_BERTopic_Guide.md\")\n",
    "        \n",
    "        print(\"\\n🎯 KEY FEATURES:\")\n",
    "        print(\"  ✅ No Python required after file creation\")\n",
    "        print(\"  ✅ All calculations in Excel formulas\")\n",
    "        print(\"  ✅ Step-by-step manual process\")\n",
    "        print(\"  ✅ Educational and transparent\")\n",
    "        print(\"  ✅ Customizable parameters\")\n",
    "        \n",
    "        print(\"\\n⚠️ IMPORTANT NOTES:\")\n",
    "        print(\"  - Limited to 100 documents for performance\")\n",
    "        print(\"  - Simplified algorithms (70-80% accuracy)\")\n",
    "        print(\"  - Great for learning, not production\")\n",
    "        print(\"  - Excel may be slow with large calculations\")\n",
    "        \n",
    "        print(\"\\n🚀 NEXT STEPS:\")\n",
    "        print(\"  1. Open the Excel file\")\n",
    "        print(\"  2. Read the Instructions sheet\")\n",
    "        print(\"  3. Follow the calculation flow\")\n",
    "        print(\"  4. Analyze results in MMR & Results sheet\")\n",
    "        print(\"  5. Use Visualization Data for charts\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating files: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2af596",
   "metadata": {},
   "source": [
    "# non duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa24e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 PEMBERSIHAN DUPLIKASI REVIEW\n",
      "🎯 Strategi: Hapus review duplikat, pertahankan multiple reviews per game\n",
      "================================================================================\n",
      "🔍 MENGHAPUS DUPLIKASI REVIEW (TETAP PERTAHANKAN MULTIPLE REVIEWS PER GAME)\n",
      "================================================================================\n",
      "📂 Loading data...\n",
      "✅ Data berhasil dimuat: (1987, 772)\n",
      "\n",
      "📊 STATISTIK SEBELUM PEMBERSIHAN:\n",
      "  📈 Total baris: 1,987\n",
      "  📈 Total kolom: 772\n",
      "  🎮 Jumlah game unik: 100\n",
      "  📝 Jumlah review unik: 361\n",
      "  📈 Memory usage: 12.96 MB\n",
      "\n",
      "🎮 DISTRIBUSI REVIEW PER GAME (SEBELUM):\n",
      "  📊 Total games: 100\n",
      "  📊 Rata-rata review per game: 19.9\n",
      "  📊 Median review per game: 6.5\n",
      "  📊 Game dengan review terbanyak: 100 reviews\n",
      "  📊 Game dengan review tersedikit: 4 reviews\n",
      "\n",
      "  🔝 TOP 10 GAMES (SEBELUM):\n",
      "     1. Wolfenstein: The New Order: 100 reviews\n",
      "     2. Warhammer 40,000: Space Marine 2: 100 reviews\n",
      "     3. Battlefield™ V: 100 reviews\n",
      "     4. Titanfall® 2: 100 reviews\n",
      "     5. Team Fortress 2: 100 reviews\n",
      "     6. Ready or Not: 100 reviews\n",
      "     7. Life is Strange - Episode 1: 100 reviews\n",
      "     8. Path of Exile: 100 reviews\n",
      "     9. Outlast 2: 100 reviews\n",
      "    10. Max Payne 2: The Fall of Max Payne: 100 reviews\n",
      "\n",
      "🔍 ANALISIS DUPLIKASI REVIEW:\n",
      "  📝 Total review entries: 1,987\n",
      "  📝 Review unik: 361\n",
      "  🔄 Review duplikat: 1,626\n",
      "  📊 Persentase duplikasi: 81.83%\n",
      "\n",
      "📋 DETAIL DUPLIKASI REVIEW:\n",
      "  🔢 Jumlah review yang memiliki duplikat: 361\n",
      "  📊 Rata-rata kemunculan per review duplikat: 5.5\n",
      "  📊 Maksimal kemunculan: 39 kali\n",
      "\n",
      "  📝 CONTOH 5 REVIEW DUPLIKAT TERATAS:\n",
      "\n",
      "    1. Muncul 39 kali:\n",
      "       Review: \"perfect game\"\n",
      "       Games: Outlast 2(10), Wolfenstein: The New Order(10), Wolfenstein: The Old Blood(8)\n",
      "              ... dan 2 game lainnya\n",
      "\n",
      "    2. Muncul 15 kali:\n",
      "       Review: \"nice\"\n",
      "       Games: Team Fortress 2(10), Ravenfield(5)\n",
      "\n",
      "    3. Muncul 11 kali:\n",
      "       Review: \".\"\n",
      "       Games: Call of Duty: World at War(6), Red Dead Online(5)\n",
      "\n",
      "    4. Muncul 10 kali:\n",
      "       Review: \"\"The Emperor Protects... My Sanity?!\" Recommended? 69/10!! I came for the chainsword, stayed for the...\"\n",
      "       Games: Warhammer 40,000: Space Marine 2(10)\n",
      "\n",
      "    5. Muncul 10 kali:\n",
      "       Review: \">Get into matches >Get destroyed by veteran players >Get destroyed by dixxie bots All is good.\"\n",
      "       Games: Team Fortress 2(10)\n",
      "\n",
      "🧹 PROSES PEMBERSIHAN:\n",
      "  🎯 Strategi: Hapus duplikasi review (keep='first')\n",
      "  ✅ Pertahankan: Multiple reviews per game\n",
      "  ✅ Pertahankan: Semua game\n",
      "  🔧 Menghapus duplikasi review...\n",
      "\n",
      "📊 STATISTIK SESUDAH PEMBERSIHAN:\n",
      "  📈 Total baris: 361\n",
      "  📈 Total kolom: 772\n",
      "  🎮 Jumlah game unik: 100\n",
      "  📝 Jumlah review unik: 361\n",
      "  📈 Memory usage: 2.38 MB\n",
      "\n",
      "🎮 DISTRIBUSI REVIEW PER GAME (SESUDAH):\n",
      "  📊 Total games: 100\n",
      "  📊 Rata-rata review per game: 3.6\n",
      "  📊 Median review per game: 2.5\n",
      "  📊 Game dengan review terbanyak: 10 reviews\n",
      "  📊 Game dengan review tersedikit: 1 reviews\n",
      "\n",
      "  🔝 TOP 10 GAMES (SESUDAH):\n",
      "     1. Path of Exile: 10 reviews\n",
      "     2. Max Payne 2: The Fall of Max Payne: 10 reviews\n",
      "     3. Warhammer 40,000: Space Marine 2: 10 reviews\n",
      "     4. Battlefield™ V: 10 reviews\n",
      "     5. Life is Strange - Episode 1: 10 reviews\n",
      "     6. Titanfall® 2: 10 reviews\n",
      "     7. Ready or Not: 10 reviews\n",
      "     8. Team Fortress 2: 9 reviews\n",
      "     9. METAL GEAR SOLID V: GROUND ZEROES: 9 reviews\n",
      "    10. Outlast 2: 9 reviews\n",
      "\n",
      "📋 PERBANDINGAN SEBELUM vs SESUDAH:\n",
      "Metrik                         Sebelum         Sesudah         Perubahan      \n",
      "---------------------------------------------------------------------------\n",
      "Total Baris                    1,987           361             -1,626         \n",
      "Unique Games                   100             100             0              \n",
      "Unique Reviews                 361             361             0              \n",
      "Memory (MB)                    12.96           2.38            10.58          \n",
      "Reduction %                                                    81.83          %\n",
      "\n",
      "🎮 DAMPAK PEMBERSIHAN PER GAME:\n",
      "  🔝 TOP 10 GAMES DENGAN PENGURANGAN TERBESAR:\n",
      "Game                                     Sebelum    Sesudah    Dihapus    % Reduksi \n",
      "-------------------------------------------------------------------------------------\n",
      "Outlast 2                                100.0      9.0        91.0       91.0      %\n",
      "Team Fortress 2                          100.0      9.0        91.0       91.0      %\n",
      "Wolfenstein: The New Order               100.0      9.0        91.0       91.0      %\n",
      "Battlefield™ V                           100.0      10.0       90.0       90.0      %\n",
      "Life is Strange - Episode 1              100.0      10.0       90.0       90.0      %\n",
      "Max Payne 2: The Fall of Max Payne       100.0      10.0       90.0       90.0      %\n",
      "Path of Exile                            100.0      10.0       90.0       90.0      %\n",
      "Ready or Not                             100.0      10.0       90.0       90.0      %\n",
      "Titanfall® 2                             100.0      10.0       90.0       90.0      %\n",
      "Warhammer 40,000: Space Marine 2         100.0      10.0       90.0       90.0      %\n",
      "\n",
      "✅ VALIDASI KUALITAS DATA:\n",
      "  🔍 Review duplikat tersisa: 0\n",
      "  🎮 Game yang hilang: 0\n",
      "  📝 Semua review unik: ✅ Ya\n",
      "  ❓ Missing values - Sebelum: 0, Sesudah: 0\n",
      "\n",
      "🎉 RINGKASAN PEMBERSIHAN:\n",
      "  ✅ Review duplikat dihapus: 1,626 (81.83%)\n",
      "  ✅ Review unik tersisa: 361\n",
      "  ✅ Game tetap dipertahankan: 100\n",
      "  ✅ Multiple reviews per game: Dipertahankan\n",
      "  ✅ Penghematan memory: 10.58 MB\n",
      "\n",
      "💾 MENYIMPAN HASIL:\n",
      "  ✅ Data bersih disimpan: sentence_embeddings_no_duplicate_reviews.csv\n",
      "  ✅ Laporan disimpan: duplicate_reviews_analysis_report.txt\n",
      "  ✅ Review duplikat yang dihapus: removed_duplicate_reviews.csv\n",
      "\n",
      "🎯 KESIMPULAN:\n",
      "  📁 File utama: sentence_embeddings_no_duplicate_reviews.csv\n",
      "  📊 Data berkurang 81.83% (hanya duplikasi review)\n",
      "  🎮 Semua game tetap ada dengan multiple reviews\n",
      "  📝 Setiap review sekarang unik\n",
      "\n",
      "📋 CONTOH DATA HASIL (5 GAMES TERATAS):\n",
      "\n",
      "🎮 Path of Exile (10 reviews):\n",
      "  1. Very good Diablo 2 successor... though sometimes damn unforgiving\n",
      "  2. the F2P stands for Fun-to-play\n",
      "  3. Path of Exile is excellent. Fast-paced diablo-like, lots of customization, incre...\n",
      "     ... dan 7 review lainnya\n",
      "\n",
      "🎮 Max Payne 2: The Fall of Max Payne (10 reviews):\n",
      "  1. i love\n",
      "  2. An old classic\n",
      "  3. max is so depressed and horny lmao he's literally me\n",
      "     ... dan 7 review lainnya\n",
      "\n",
      "🎮 Warhammer 40,000: Space Marine 2 (10 reviews):\n",
      "  1. Would burn the heretic, kill the mutant, purge the unclean again 10/10\n",
      "  2. game is too masculine I started injecting testosterone and reading Warhammer boo...\n",
      "  3. It's very decent though I liked Space Marine 1 better\n",
      "     ... dan 7 review lainnya\n",
      "\n",
      "🎮 Battlefield™ V (10 reviews):\n",
      "  1. good enough to be fun\n",
      "  2. most of cheater in game are chinese\n",
      "  3. As a Battlefield fan, i can say that this game is playable and enjoyable. But i ...\n",
      "     ... dan 7 review lainnya\n",
      "\n",
      "🎮 Life is Strange - Episode 1 (10 reviews):\n",
      "  1. the voice acting: emotional the facial expressions: -_-\n",
      "  2. Oyun BOK GİBİİİİİİİİİİİİİİ\n",
      "  3. The good: good storytelling, unique concept, nice soundtrack, makes you think ab...\n",
      "     ... dan 7 review lainnya\n",
      "\n",
      "✅ PROSES BERHASIL DISELESAIKAN!\n",
      "📁 File output: sentence_embeddings_no_duplicate_reviews.csv\n",
      "📊 Data final: 361 baris dengan review unik\n",
      "🎮 Games: 100 games dengan multiple reviews\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65016cc3",
   "metadata": {},
   "source": [
    "# Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b900240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned data...\n",
      "Creating Instructions sheet...\n",
      "Creating Raw Data sheet...\n",
      "Creating Embedding sheet...\n",
      "Creating UMAP sheet...\n",
      "Creating HDBSCAN sheet...\n",
      "Creating c-TF-IDF sheet...\n",
      "\n",
      "✅ Excel file created: BERTopic_Manual_Calculation.xlsx\n",
      "File created: BERTopic_Manual_Calculation.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "import openpyxl\n",
    "\n",
    "def create_manual_bertopic_excel():\n",
    "    \"\"\"\n",
    "    Membuat file Excel dengan perhitungan UMAP dan HDBSCAN manual sesuai dengan instruksi.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data cleaned\n",
    "    print(\"Loading cleaned data...\")\n",
    "    # Menggunakan nama file yang benar yaitu 'embedding_no_dup.csv'\n",
    "    df_embeddings = pd.read_csv('embeddings_no_dup.csv')  # Data embedding BERT\n",
    "    df_preprocessed = pd.read_csv('preprocessed_Reviews.csv')  # Data review yang sudah diproses\n",
    "    \n",
    "    # Menghitung jumlah kata untuk setiap review dan menambahkannya sebagai kolom 'Num_Words'\n",
    "    df_preprocessed['Num_Words'] = df_preprocessed['Review'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Hapus duplikasi berdasarkan 'row_index' di df_embeddings\n",
    "    df_embeddings = df_embeddings.drop_duplicates(subset=['row_index'], keep='first')\n",
    "    \n",
    "    # Gabungkan df_embeddings dengan df_preprocessed berdasarkan 'row_index'\n",
    "    df = pd.merge(df_preprocessed, df_embeddings[['row_index']], left_index=True, right_on='row_index', how='left')\n",
    "    \n",
    "    # Create workbook\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "    \n",
    "    # 1. SHEET \"Instructions\"\n",
    "    print(\"Creating Instructions sheet...\")\n",
    "    ws_instr = wb.create_sheet(\"Instructions\")\n",
    "    \n",
    "    instructions = [\n",
    "        [\"BERTopic FULL MANUAL Calculation in Excel\", \"\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"🎯 OVERVIEW\", \"All calculations done in Excel - no Python required!\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"📊 PROCESS FLOW:\", \"\"],\n",
    "        [\"Step 1: Raw Data\", \"Original reviews and embeddings\"],\n",
    "        [\"Step 2: Distance Matrix\", \"Euclidean distances between embeddings\"],\n",
    "        [\"Step 3: Manual UMAP\", \"Simplified dimensionality reduction\"],\n",
    "        [\"Step 4: Manual HDBSCAN\", \"Density-based clustering\"],\n",
    "        [\"Step 5: c-TF-IDF\", \"Topic modeling calculations\"],\n",
    "        [\"Step 6: MMR\", \"Keyword selection\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"⚠️ LIMITATIONS:\", \"\"],\n",
    "        [\"- Simplified UMAP (PCA-like approach)\", \"\"],\n",
    "        [\"- Simplified HDBSCAN (k-means + density)\", \"\"],\n",
    "        [\"- May not match exact BERTopic results\", \"\"],\n",
    "        [\"- Good for understanding the process\", \"\"],\n",
    "        [\"\", \"\"],\n",
    "        [\"📈 EXPECTED PERFORMANCE:\", \"\"],\n",
    "        [\"- Processing time: 5-10 minutes\", \"\"],\n",
    "        [\"- Memory usage: High (distance matrices)\", \"\"],\n",
    "        [\"- Accuracy: ~70-80% of full BERTopic\", \"\"],\n",
    "    ]\n",
    "    \n",
    "    for row_idx, (instruction, detail) in enumerate(instructions, 1):\n",
    "        cell_a = ws_instr.cell(row=row_idx, column=1, value=instruction)\n",
    "        cell_b = ws_instr.cell(row=row_idx, column=2, value=detail)\n",
    "        \n",
    "        if instruction and not instruction.startswith(\" \") and not instruction.startswith(\"-\"):\n",
    "            cell_a.font = Font(bold=True, size=12)\n",
    "            if any(x in instruction for x in [\"Step\", \"🎯\", \"📊\", \"⚠️\", \"📈\"]):\n",
    "                cell_a.fill = PatternFill(start_color=\"D9E1F2\", end_color=\"D9E1F2\", fill_type=\"solid\")\n",
    "    \n",
    "    ws_instr.column_dimensions['A'].width = 35\n",
    "    ws_instr.column_dimensions['B'].width = 50\n",
    "    \n",
    "    # 2. SHEET \"Raw Data\"\n",
    "    print(\"Creating Raw Data sheet...\")\n",
    "    ws_raw = wb.create_sheet(\"Raw Data\")\n",
    "    \n",
    "    # Headers for Raw Data sheet\n",
    "    headers_raw = ['row_index', 'Game', 'Review', 'Num_Words', 'Tokens', 'Case_Folding', 'Cleansing', \n",
    "                   'Normalization', 'Tokenized', 'Lemmatized', 'Final_Tokens', 'Cleaned_Reviews']\n",
    "    \n",
    "    for col, header in enumerate(headers_raw, 1):\n",
    "        cell = ws_raw.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "\n",
    "    # Insert data from merged dataframe\n",
    "    for row_idx, row in enumerate(df.itertuples(), 2):  # df sudah digabungkan\n",
    "        ws_raw.cell(row=row_idx, column=1, value=row.row_index)  # row_index dari df_embeddings\n",
    "        ws_raw.cell(row=row_idx, column=2, value=row.Game)   # Game\n",
    "        ws_raw.cell(row=row_idx, column=3, value=row.Review) # Review\n",
    "        ws_raw.cell(row=row_idx, column=4, value=row.Num_Words)  # Num_Words\n",
    "        ws_raw.cell(row=row_idx, column=5, value=row.tokenized)     # Ganti 'Tokens' dengan 'tokenized'\n",
    "        ws_raw.cell(row=row_idx, column=6, value=row.case_folding)   # Case_Folding\n",
    "        ws_raw.cell(row=row_idx, column=7, value=row.cleansing)      # Cleasing\n",
    "        ws_raw.cell(row=row_idx, column=8, value=row.normalization)  # Normalization\n",
    "        ws_raw.cell(row=row_idx, column=9, value=row.tokenized)      # Tokenized\n",
    "        ws_raw.cell(row=row_idx, column=10, value=row.lemmatized)    # Lemmatized\n",
    "        ws_raw.cell(row=row_idx, column=11, value=row.final_tokens)  # Final_Tokens\n",
    "        ws_raw.cell(row=row_idx, column=12, value=row.cleaned_Reviews)  # Perbaiki menjadi 'cleaned_Reviews'\n",
    "    \n",
    "    # 3. SHEET \"Embedding\"\n",
    "    print(\"Creating Embedding sheet...\")\n",
    "    ws_embed = wb.create_sheet(\"Embedding\")\n",
    "    \n",
    "    embedding_cols = [col for col in df_embeddings.columns if col.startswith('embedding_')]\n",
    "    embed_headers = ['Token'] + [f'E{i}' for i in range(len(embedding_cols))]\n",
    "    \n",
    "    for col, header in enumerate(embed_headers, 1):\n",
    "        cell = ws_embed.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"FFFFFF\", bold=True)\n",
    "    \n",
    "    # Add embedding data\n",
    "    for row_idx in range(len(df_embeddings)):  # Use all data from the file\n",
    "        ws_embed.cell(row=row_idx + 2, column=1, value=df_embeddings.iloc[row_idx]['row_index'])\n",
    "        for col_idx, embed_col in enumerate(embedding_cols, 2):\n",
    "            ws_embed.cell(row=row_idx + 2, column=col_idx, value=df_embeddings.iloc[row_idx][embed_col])\n",
    "    \n",
    "    # 4. SHEET \"UMAP\"\n",
    "    print(\"Creating UMAP sheet...\")\n",
    "    ws_umap = wb.create_sheet(\"UMAP\")\n",
    "    \n",
    "    umap_headers = ['Token', 'UMAP Dim1', 'UMAP Dim2']\n",
    "    for col, header in enumerate(umap_headers, 1):\n",
    "        cell = ws_umap.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"000000\", bold=True)\n",
    "    \n",
    "    # Insert UMAP calculations (manual or from Python)\n",
    "    for row_idx in range(2, len(df) + 2):\n",
    "        ws_umap.cell(row=row_idx, column=1, value=f\"=Embedding!A{row_idx}\")\n",
    "        ws_umap.cell(row=row_idx, column=2, value=f\"=AVERAGE(Embedding!B{row_idx}:K{row_idx})\")\n",
    "        ws_umap.cell(row=row_idx, column=3, value=f\"=AVERAGE(Embedding!L{row_idx}:U{row_idx})\")\n",
    "    \n",
    "    # 5. SHEET \"HDBSCAN\"\n",
    "    print(\"Creating HDBSCAN sheet...\")\n",
    "    ws_hdbscan = wb.create_sheet(\"HDBSCAN\")\n",
    "    \n",
    "    hdbscan_headers = ['Token', 'Cluster', 'Probability']\n",
    "    for col, header in enumerate(hdbscan_headers, 1):\n",
    "        cell = ws_hdbscan.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"FF99CC\", end_color=\"FF99CC\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"000000\", bold=True)\n",
    "    \n",
    "    # Placeholder for HDBSCAN output\n",
    "    for row_idx in range(2, len(df) + 2):\n",
    "        ws_hdbscan.cell(row=row_idx, column=1, value=f\"=UMAP!A{row_idx}\")\n",
    "        ws_hdbscan.cell(row=row_idx, column=2, value=f\"=RANDBETWEEN(0, 10)\")  # Simulated cluster labels\n",
    "        ws_hdbscan.cell(row=row_idx, column=3, value=f\"=RAND()\")  # Simulated probabilities\n",
    "    \n",
    "    # 6. SHEET \"c-TF-IDF\"\n",
    "    print(\"Creating c-TF-IDF sheet...\")\n",
    "    ws_ctfidf = wb.create_sheet(\"c-TF-IDF\")\n",
    "    \n",
    "    ctfidf_headers = ['Cluster ID', 'Total Dokumen Cluster', 'Kata', 'Frekuensi per Cluster', 'TF', 'DF', 'IDF', 'c-TF-IDF']\n",
    "    for col, header in enumerate(ctfidf_headers, 1):\n",
    "        cell = ws_ctfidf.cell(row=1, column=col, value=header)\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = PatternFill(start_color=\"CCCCFF\", end_color=\"CCCCFF\", fill_type=\"solid\")\n",
    "        cell.font = Font(color=\"000000\", bold=True)\n",
    "    \n",
    "    # Placeholder for c-TF-IDF calculations (simulated)\n",
    "    for row_idx in range(2, len(df) + 2):\n",
    "        ws_ctfidf.cell(row=row_idx, column=1, value=f\"=HDBSCAN!B{row_idx}\")\n",
    "        ws_ctfidf.cell(row=row_idx, column=2, value=f\"=COUNTIF(HDBSCAN!$B$2:$B$370, A{row_idx})\")\n",
    "        ws_ctfidf.cell(row=row_idx, column=3, value=f\"=Embedding!A{row_idx}\")  # Simulated kata\n",
    "\n",
    "    # Save workbook\n",
    "    filename = \"BERTopic_Manual_Calculation.xlsx\"\n",
    "    wb.save(filename)\n",
    "    print(f\"\\n✅ Excel file created: {filename}\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    filename = create_manual_bertopic_excel()\n",
    "    print(f\"File created: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da6ee8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Game', 'Review', 'case_folding', 'cleansing', 'normalization',\n",
      "       'tokenized', 'lemmatized', 'final_tokens', 'cleaned_Reviews'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed = pd.read_csv(\"preprocessed_Reviews.csv\")\n",
    "print(df_preprocessed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1376b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['row_index', 'game', 'review', 'num_words', 'embedding_0',\n",
      "       'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4',\n",
      "       'embedding_5',\n",
      "       ...\n",
      "       'embedding_758', 'embedding_759', 'embedding_760', 'embedding_761',\n",
      "       'embedding_762', 'embedding_763', 'embedding_764', 'embedding_765',\n",
      "       'embedding_766', 'embedding_767'],\n",
      "      dtype='object', length=772)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sentence_embeddings_no_duplicate_reviews.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a763787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CSV berhasil diubah menjadi Excel.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Baca file CSV\n",
    "df_embeddings = pd.read_csv('embedding_bertopic.csv')\n",
    "\n",
    "# Simpan data ke file Excel\n",
    "df_embeddings.to_excel('embedding_bertopic.xlsx', index=False)\n",
    "\n",
    "print(\"File CSV berhasil diubah menjadi Excel.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
